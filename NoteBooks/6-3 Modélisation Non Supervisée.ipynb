{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "import datetime\n",
    "nltk.data.path.append(r\"/Users/seb/Workspace/Dev/Formation-OC/LIBRAIRIES/nltk_data\")\n",
    "\n",
    "import nltk\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'data/QueryResults3_new.csv'\n",
    "df1 = pd.read_csv(file1,index_col=['Id'])\n",
    "\n",
    "#import mes_fonctions_final as stt\n",
    "import mes_fonctions_final2 as stt\n",
    "import os\n",
    "\n",
    "StopWord = stt.load_stop_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mes_fonctions_final2' from '/Users/seb/Workspace/Dev/Formation-OC/Formation-Data-Scientist/Projet6-Categoriser_Automatiquement_des_Questions/NoteBooks/PROJET_A_LIVRER/mes_fonctions_final2.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(stt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappel des Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "def preProcess_remove_Code(html_in):\n",
    "    # Chargement du module BeautifulSoup pour le parsing des données HTML\n",
    "    from bs4 import BeautifulSoup\n",
    "    html_in_soup = BeautifulSoup(html_in, 'html.parser')\n",
    "\n",
    "    # Suppression des blocs de Code\n",
    "    for h_code in html_in_soup.find_all('code'):\n",
    "        h_code.decompose()\n",
    "\n",
    "    return html_in_soup.get_text().lower()\n",
    "\n",
    "\n",
    "def preProcess_keep_Code(html_in):\n",
    "    # Chargement du module BeautifulSoup pour le parsing des données HTML\n",
    "    from bs4 import BeautifulSoup\n",
    "    html_in_soup = BeautifulSoup(html_in, 'html.parser')\n",
    "\n",
    "    return html_in_soup.get_text().lower()\n",
    "\n",
    "# Le préprocessing :\n",
    "#   - nettoie le html\n",
    "#   - supprime les blocs de Code\n",
    "#   - ne prend que la partie textuelle du html\n",
    "#   - renvoie ce texte en minuscule\n",
    "\n",
    "\n",
    "def preProcess(html_in):\n",
    "    # Chargement du module BeautifulSoup pour le parsing des données HTML\n",
    "    from bs4 import BeautifulSoup\n",
    "    html_in_soup = BeautifulSoup(html_in, 'html.parser')\n",
    "\n",
    "    # Suppression des blocs de Code\n",
    "    for h_code in html_in_soup.find_all('code'):\n",
    "        h_code.decompose()\n",
    "\n",
    "    return html_in_soup.get_text().lower()\n",
    "\n",
    "\n",
    "# Réalisation du stemming (on coupe les racines)\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "# La tokenisation :\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    m_token_pattern = r\"((?:(?:(?:[0-9a-zA-Z])\\.){2,}[a-zA-Z])\" +\\\n",
    "        \"|(?:(?:[0-9a-zA-Z]){2,}\\.(?:[0-9a-zA-Z]){2,}\" +\\\n",
    "        \"|(?:\\.(?:[0-9a-zA-Z]){2,}))\" +\\\n",
    "        \"|[0-9a-zA-Z-\\-\\+\\#]{2,}|w+)\"\n",
    "\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    # Nous allons utiliser le pattern pour identifier les mots\n",
    "    tokenizer = RegexpTokenizer(m_token_pattern)\n",
    "\n",
    "    # Nous lançons la séparation des mots\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # On fait appel au stemming pour rapprocher les mots de même racine\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "\n",
    "    # Etape de nettoyage des valeurs :\n",
    "    # Nous allons supprimer les nombres sans texte,\n",
    "    def suppress_nb(x):\n",
    "        import re\n",
    "        if x is None:\n",
    "            return None\n",
    "        pattern = r'(^[\\#\\-\\+]*[0-9]*$|' + '^[0-9]*[\\#\\-\\+]*$|'+'^[0-9]*[\\#\\-\\+]?[0-9]*$|'+'^[0-9\\#\\-\\+][a-z]$|'+'^[a-z][0-9\\#\\-\\+]$|'+'^[0-9]*\\.[0-9]*$)'\n",
    "        if not(re.match(pattern, x)):\n",
    "            return x\n",
    "\n",
    "    def nettoie_points(x):\n",
    "        import re\n",
    "        if x is None:\n",
    "            return None\n",
    "\n",
    "        if (re.match(r'(^[\\.\\-\\#][a-z]*$)', x)):\n",
    "            return ''.join(list(x)[1:])\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    # Nous filtrons les nombres seuls\n",
    "    stems = list(filter(lambda x: suppress_nb(x), stems))\n",
    "    stems = [nettoie_points(x) for x in stems]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fabrication des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/!\\ Ajout des features en cours\n"
     ]
    }
   ],
   "source": [
    "Y_tags, Y_tags_voc = stt.genere_target_dummy_and_vocabulary(data=df1, min_df=300)\n",
    "Tags_freq = stt.genere_df_target_tags_freq(data=df1, min_df=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin Train_Split : 0:00:00.055921\n"
     ]
    }
   ],
   "source": [
    "# On sépare les données d'entrainement de celles de tests\n",
    "debut = datetime.datetime.now()\n",
    "X = df1['Body']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_tags, test_size=0.2, random_state=42 )\n",
    "fin = datetime.datetime.now()\n",
    "print(\"Fin Train_Split : {}\".format(fin-debut))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions supplémentaires\n",
    "Etant donné que les tags sont multi valués, nous allons \n",
    "- créer des groupes de tags\n",
    "- limiter le nombre de tags pour les groupes représentant une faible fréquence\n",
    "- nous mettons un seuil de couverture par les groupes créés\n",
    "- nous encodons les multiples valeurs des tags\n",
    "- nous pouvons aussi décoder ceci "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fabrique_nom(row):\n",
    "        msg = ''\n",
    "        for i in row.index.tolist():\n",
    "            msg+=str(row[i])\n",
    "        return msg\n",
    "    \n",
    "def genere_encodage(seuil=0.8, Y_tags=Y_tags):\n",
    "    global dict_conversion\n",
    "    global conversion\n",
    "    global m_colonne\n",
    " \n",
    "    def fabrique_nom(row):\n",
    "        msg = ''\n",
    "        for i in row.index.tolist():\n",
    "            msg+=str(row[i])\n",
    "        return msg\n",
    "\n",
    "    def verif_seuil(row, seuil, remplacement=-1):\n",
    "        if row['CumSum_Norm']<seuil:\n",
    "            return row.name \n",
    "        else:\n",
    "            return remplacement\n",
    "\n",
    "    tmp1 = Y_tags.to_dense()\n",
    "    m_colonne = tmp1.columns.tolist()\n",
    "    tmp1 = tmp1.groupby(by=m_colonne).size().reset_index(name='Freq')\n",
    "\n",
    "    tmp1.sort_values(by=['Freq'], ascending=False, inplace=True)\n",
    "\n",
    "    name1 = tmp1[m_colonne].apply(lambda row: fabrique_nom(row=row), axis=1)\n",
    "    from sklearn import preprocessing\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    tmp2 = le.fit_transform(name1)\n",
    "\n",
    "    tmp1['Name'] = name1\n",
    "    tmp1['Value1'] = tmp2\n",
    "\n",
    "    # On ajoute la somme cumulée normalisée\n",
    "    tmp1['CumSum_Norm'] = tmp1['Freq'].cumsum()/(tmp1['Freq'].sum())\n",
    "\n",
    "\n",
    "    ## On transforme l'id selon le seuil de Fréquence du groupe\n",
    "    tmp = tmp1.apply(lambda row: verif_seuil(row, seuil=seuil), axis=1)\n",
    "\n",
    "    # On encode les noms \n",
    "    le2 = preprocessing.LabelEncoder()\n",
    "    tmp2 = le2.fit_transform(tmp)\n",
    "\n",
    "    tmp1['index_New'] = tmp2\n",
    "\n",
    "    dict_conversion = tmp1\n",
    "    ########del tmp1\n",
    "\n",
    "\n",
    "    ## GESTION DE LA TRANSFORMATION\n",
    "\n",
    "    # On fait la conversion entre les tags en INPUT et la valeur encodée\n",
    "    # Le dictionnaire est le suivant\n",
    "\n",
    "    conversion = {'INPUT': dict_conversion.Name}\n",
    "    #####conversion['OUTPUT'] = np.array(u2.loc[le.transform(conversion['INPUT'])]['index_New'])\n",
    "    conversion['OUTPUT'] = dict_conversion.index_New\n",
    "\n",
    "    tmp_df = pd.DataFrame(conversion)\n",
    "    conversion = tmp_df[np.logical_not(tmp_df.duplicated(subset=['OUTPUT'], keep='first'))]\n",
    "\n",
    "    \n",
    "def tags_binaire_2_value(value_in):\n",
    "    v1 = conversion.set_index(keys=['INPUT'])\n",
    "    return v1.loc[value_in, 'OUTPUT']\n",
    "\n",
    "def value_to_tagsBinaires(value_in):\n",
    "    v1 = conversion.set_index(keys=['OUTPUT'])\n",
    "    return v1.loc[value_in, 'INPUT']\n",
    "\n",
    "def value_to_matrix_tags(value_in):\n",
    "    '''value_to_matrix_tags([0, 1])'''\n",
    "    binary_value = value_to_tagsBinaires(value_in)\n",
    "    v2 = dict_conversion.set_index(keys=['Name'])\n",
    "    return pd.DataFrame(v2.loc[binary_value][m_colonne])\n",
    "\n",
    "def encode_Y(data_in):\n",
    "    u = data_in.apply(lambda row: fabrique_nom(row=row), axis=1)\n",
    "    valeur_remplacement = dict_conversion.head(1)['index_New'][0]\n",
    "    return pd.Series(data=tags_binaire_2_value(u).fillna(value=valeur_remplacement).values, \n",
    "                     index=data_in.index)\n",
    "\n",
    "decode_Y_pred = value_to_matrix_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de mesure de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mesure_performance(y_pred_encoded, Y_test):\n",
    "    m_Y_PRED = decode_Y_pred(y_pred_encoded)\n",
    "    m_Y_PRED.index = Y_test.index\n",
    "    m_res = Y_test==m_Y_PRED\n",
    "    print('\\tPERFORMANCE MODELE : {}  -- soit {} %'.format(\n",
    "        np.mean(np.sum(m_res, axis=1)), round(np.mean(np.sum(m_res, axis=1))/58.0, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fabrication du Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin TfidfVectorizer : 0:03:13.854340\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "vect_tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    min_df=100,\n",
    "    max_df=0.2,\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preProcess_keep_Code,\n",
    "    stop_words=StopWord,        # Pas de stopWord car nous les cherchons\n",
    "    ngram_range=(1, 6)\n",
    ")\n",
    "\n",
    "debut = datetime.datetime.now()\n",
    "X_train2 = vect_tfidf.fit_transform(X_train)\n",
    "X_test2 = vect_tfidf.transform(X_test)\n",
    "fin = datetime.datetime.now()\n",
    "print(\"Fin TfidfVectorizer : {}\".format(fin-debut))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conservation de copies des données et des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement d'un lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sur le body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tps de lda 0:00:31.457904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "no_topics = 20\n",
    "\n",
    "# Run LDA\n",
    "debut = datetime.datetime.now()\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=no_topics, max_iter=5,\n",
    "    learning_method='online', learning_offset=50.,random_state=0).fit(X_train2)\n",
    "\n",
    "fin = datetime.datetime.now()\n",
    "print('Tps de lda {}'.format(fin-debut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "\t date td tabl tr month th id day td td row\n",
      "Topic 1:\n",
      "\t python lib line usr import site-packag modul instal python3 bin\n",
      "Topic 2:\n",
      "\t public string void privat overrid public void int public class overrid public intent\n",
      "Topic 3:\n",
      "\t map option select countri option option marker lat locat latitud longitud\n",
      "Topic 4:\n",
      "\t sub dim sheet end cell excel rang worksheet vba row\n",
      "Topic 5:\n",
      "\t angular compon import observ ts export export class subscrib ionic pipe\n",
      "Topic 6:\n",
      "\t self func branch def let anim init nil tableview git\n",
      "Topic 7:\n",
      "\t imag color width height img style posit div left chart\n",
      "Topic 8:\n",
      "\t pdf notif chrome selenium firefox browser mobil webdriv paper safari\n",
      "Topic 9:\n",
      "\t system.out println system.out println public string static int void public static static void\n",
      "Topic 10:\n",
      "\t ifram keyboard timer focus delay mous true return tap creation id return\n",
      "Topic 11:\n",
      "\t div button div class div div click span form li input id\n",
      "Topic 12:\n",
      "\t app server project applic instal build version test window connect\n",
      "Topic 13:\n",
      "\t script html js src head css bodi script src href js script\n",
      "Topic 14:\n",
      "\t id request var string url json respons email type post\n",
      "Topic 15:\n",
      "\t column list number array string print row output tabl input\n",
      "Topic 16:\n",
      "\t int std android char printf includ cout layout android layout struct\n",
      "Topic 17:\n",
      "\t org.springframework xml org.apach groupid xmlns artifactid jar version http depend\n",
      "Topic 18:\n",
      "\t com.android java.lang intern android.app invok activitythread android.app activitythread 2018-10-10 activitythread.java android.o\n",
      "Topic 19:\n",
      "\t nan bucket myapp somehow import sys red tabl id scale id return especi\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\"\\t\",\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, vect_tfidf.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On constate que les topics sont bien marqués\n",
    "- le topic 1 est du python\n",
    "- le topic 18 de l'android\n",
    "- le topic 0 du html etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation par NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour le Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tps de NMF BODY 0:00:09.985290\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# On fait la transfo Tf-Idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "debut = datetime.datetime.now()\n",
    "\n",
    "# Attention nous ne voulons pas la forme TFiDF donc use_idf=False\n",
    "#tf = TfidfTransformer(use_idf=True)\n",
    "#train_data_features_fitted_body = tf.fit_transform(X_body_stt)\n",
    "\n",
    "#tfidf_feature_names_body = X_body_stt.columns.tolist()\n",
    "no_topics = 20\n",
    "\n",
    "debut = datetime.datetime.now()\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(\n",
    "    X_train2)\n",
    "\n",
    "fin = datetime.datetime.now()\n",
    "print('Tps de NMF BODY {}'.format(fin-debut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "\t instal project version build app node modul packag command applic\n",
      "Topic 1:\n",
      "\t div div class div div span div div class div div div li href width style\n",
      "Topic 2:\n",
      "\t string public void privat public void public class static overrid null public string\n",
      "Topic 3:\n",
      "\t id id id select queri id name product order group null item\n",
      "Topic 4:\n",
      "\t print line number output end def rang input result string\n",
      "Topic 5:\n",
      "\t android layout android layout app parent layout height layout width android layout width android layout height activ\n",
      "Topic 6:\n",
      "\t imag img upload width imag descript height size enter imag descript enter imag descript\n",
      "Topic 7:\n",
      "\t server connect client port databas host password sql socket send\n",
      "Topic 8:\n",
      "\t import compon const modul export react angular prop path render\n",
      "Topic 9:\n",
      "\t button click page view text event style show html item\n",
      "Topic 10:\n",
      "\t date month format day date date year datetim date format mm start date\n",
      "Topic 11:\n",
      "\t int std int int includ char printf cout i++ main void\n",
      "Topic 12:\n",
      "\t var console.log script true let err document.getelementbyid fals func js\n",
      "Topic 13:\n",
      "\t tabl column row select queri sql databas insert df null\n",
      "Topic 14:\n",
      "\t type form input field text email post input type label option\n",
      "Topic 15:\n",
      "\t list item list list element list string object list item index item list iter\n",
      "Topic 16:\n",
      "\t td tr td td th td tr th th tabl tr td tbodi tr tr\n",
      "Topic 17:\n",
      "\t request url api http https respons com json header token\n",
      "Topic 18:\n",
      "\t test test test train unit unit test test case fail mock expect case\n",
      "Topic 19:\n",
      "\t array array array element object arr index key let php item\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "display_topics(nmf, vect_tfidf.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapes\n",
    "1. Nous fabriquons les noms des Groupes sur chaque POST\n",
    "2. Nous comptons la fréquence de chaque groupe\n",
    "3. Nous identifions le nombre 'négligeables' de groupes\n",
    "4. Nous pouvons lancer la modélisation lda qui va générer le nombre de topics correspondant au nombre de groupe\n",
    "5. On modélise l'appartenance un topic vers un groupe\n",
    "6. On prédit un groupe de tags à partir de la transformation LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelisation_LDA(clf, X_train2, X_test2, Y_train, Y_test, seuil_groupe_tags=0.8):\n",
    "    debut0 = datetime.datetime.now()\n",
    "    debut = datetime.datetime.now()\n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2)\n",
    "    \n",
    "    # Generation Encodage \n",
    "    genere_encodage(seuil=seuil_groupe_tags)\n",
    "    \n",
    "    # 0 - Encodage des Y selon le seuil\n",
    "    Y_train_name_encode = encode_Y(Y_train)\n",
    "    Y_test_name_encode = encode_Y(Y_test)\n",
    "\n",
    "    # 1 - Transfo TF-IDF VECTORIZER\n",
    "    nb_topics = Y_train_name_encode.nunique()\n",
    "    print('nb_topics = ', nb_topics)\n",
    "    \n",
    "    # On entraine la modelisation LDA\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    from sklearn.decomposition import NMF\n",
    "    \n",
    "    # On entraine la modelisation LDA\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=nb_topics, max_iter=5,\n",
    "        learning_method='online', learning_offset=20.,random_state=0, \n",
    "        doc_topic_prior=0.1 , topic_word_prior=0.01\n",
    "    )\n",
    "    \n",
    "    debut = datetime.datetime.now()\n",
    "    X_train_lda_trained = lda.fit_transform(X_train2)\n",
    "    fin = datetime.datetime.now()\n",
    "    print('Tps de Traitement : X_train_lda_trained\\t{}'.format(fin-debut))\n",
    "    \n",
    "    debut = datetime.datetime.now()\n",
    "    # On tranforme la données de test\n",
    "    X_test_lda_trained = lda.fit_transform(X_test2)\n",
    "    fin = datetime.datetime.now()\n",
    "    print('Tps de Traitement : X_test_lda_trained\\t{}'.format(fin-debut))\n",
    "    \n",
    "    debut = datetime.datetime.now()\n",
    "    print('\\t[ ETAPE3 ]')\n",
    "\n",
    "    ## Entrainement par le Classifier\n",
    "    clf.fit(X_train_lda_trained, Y_train_name_encode)\n",
    "    fin = datetime.datetime.now()\n",
    "    print('Tps de Traitement : clf.fit\\t{}'.format(fin-debut))\n",
    "    debut = datetime.datetime.now()\n",
    "\n",
    "    # On mesure le score\n",
    "    score_clf = clf.score(X_test_lda_trained, Y_test_name_encode)\n",
    "    print('\\t-> score = ', score_clf)\n",
    "    \n",
    "    fin = datetime.datetime.now()\n",
    "    print('Tps de Traitement : SCORE\\t{}'.format(fin-debut))\n",
    "    print('\\nTps Total {}'.format(fin-debut0))\n",
    "    \n",
    "    return clf, clf.predict(X_test_lda_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premier test via LDA avec une Régression Logistic\n",
    "> Nous mettons un seuil de 0.8 (donc si les groupes pris en compte représentent 80 % des groupes possibles de tags, alors nous ne prenons pas les autres en compte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_topics =  66\n",
      "Tps de Traitement : X_train_lda_trained\t0:00:45.115857\n",
      "Tps de Traitement : X_test_lda_trained\t0:00:10.327847\n",
      "\t[ ETAPE3 ]\n",
      "Tps de Traitement : clf.fit\t0:00:04.713412\n",
      "\t-> score =  0.4346\n",
      "Tps de Traitement : SCORE\t0:00:00.009389\n",
      "\n",
      "Tps Total 0:01:19.247862\n",
      "\tPERFORMANCE MODELE : 56.8531  -- soit 0.9802 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf1 = LogisticRegression(C=0.10, penalty='l2', tol=0.1,random_state=0, \n",
    "                          solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "n_clf, y_pred_encoded = modelisation_LDA(\n",
    "    clf1, \n",
    "    X_train2=X_train2, X_test2=X_test2, Y_train=Y_train, Y_test=Y_test,\n",
    "    seuil_groupe_tags=0.8\n",
    ")\n",
    "\n",
    "mesure_performance(y_pred_encoded, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> La performance est la moyenne des tags qui sont bons pour chaque POST : ayant 58 tags, c'est la valeur maximale que nous pourrions obtenir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baissons le seuil pour le nombre de groupe à 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_topics =  37\n",
      "Tps de Traitement : X_train_lda_trained\t0:00:38.504661\n",
      "Tps de Traitement : X_test_lda_trained\t0:00:07.448606\n",
      "\t[ ETAPE3 ]\n",
      "Tps de Traitement : clf.fit\t0:00:02.298049\n",
      "\t-> score =  0.5379\n",
      "Tps de Traitement : SCORE\t0:00:00.002227\n",
      "\n",
      "Tps Total 0:01:07.152423\n",
      "\tPERFORMANCE MODELE : 56.853  -- soit 0.9802 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf1 = LogisticRegression(C=0.10, penalty='l2', tol=0.1,random_state=0, \n",
    "                          solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "n_clf, y_pred_encoded = modelisation_LDA(\n",
    "    clf1, \n",
    "    X_train2=X_train2, X_test2=X_test2, Y_train=Y_train, Y_test=Y_test,\n",
    "    seuil_groupe_tags=0.7\n",
    ")\n",
    "\n",
    "mesure_performance(y_pred_encoded, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maintenant augmentons ce seuil à 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_topics =  368\n",
      "Tps de Traitement : X_train_lda_trained\t0:02:30.225838\n",
      "Tps de Traitement : X_test_lda_trained\t0:00:33.916603\n",
      "\t[ ETAPE3 ]\n",
      "Tps de Traitement : clf.fit\t0:00:48.512405\n",
      "\t-> score =  0.283\n",
      "Tps de Traitement : SCORE\t0:00:00.064979\n",
      "\n",
      "Tps Total 0:04:12.146149\n",
      "\tPERFORMANCE MODELE : 56.8531  -- soit 0.9802 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf1 = LogisticRegression(C=0.10, penalty='l2', tol=0.1,random_state=0, \n",
    "                          solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "n_clf, y_pred_encoded = modelisation_LDA(\n",
    "    clf1, \n",
    "    X_train2=X_train2, X_test2=X_test2, Y_train=Y_train, Y_test=Y_test,\n",
    "    seuil_groupe_tags=0.95\n",
    ")\n",
    "\n",
    "mesure_performance(y_pred_encoded, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> La performance est très proche pour un temps d'entrainement plus long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Faisons évoluer les hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_topics =  66\n",
      "Tps de Traitement : X_train_lda_trained\t0:00:47.045701\n",
      "Tps de Traitement : X_test_lda_trained\t0:00:11.633321\n",
      "\t[ ETAPE3 ]\n",
      "Tps de Traitement : clf.fit\t0:00:06.051731\n",
      "\t-> score =  0.1868\n",
      "Tps de Traitement : SCORE\t0:00:00.008952\n",
      "\n",
      "Tps Total 0:01:24.576898\n",
      "\tPERFORMANCE MODELE : 55.9436  -- soit 0.9645 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf1 = LogisticRegression(C=10, penalty='l2', tol=0.01,random_state=0, \n",
    "                          solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "n_clf, y_pred_encoded = modelisation_LDA(\n",
    "    clf1, \n",
    "    X_train2=X_train2, X_test2=X_test2, Y_train=Y_train, Y_test=Y_test,\n",
    "    seuil_groupe_tags=0.80\n",
    ")\n",
    "\n",
    "mesure_performance(y_pred_encoded, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Le score baisse fortement avec les modifications des hyper paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin TfidfVectorizer : 0:03:20.278255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "vect_count = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    min_df=100,\n",
    "    max_df=0.2,\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preProcess_keep_Code,\n",
    "    stop_words=StopWord,        # Pas de stopWord car nous les cherchons\n",
    "    ngram_range=(1, 6)\n",
    ")\n",
    "\n",
    "debut = datetime.datetime.now()\n",
    "X_train2 = vect_count.fit_transform(X_train)\n",
    "X_test2 = vect_count.transform(X_test)\n",
    "fin = datetime.datetime.now()\n",
    "print(\"Fin TfidfVectorizer : {}\".format(fin-debut))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelisation_MNF(clf, X_train2, X_test2, Y_train, Y_test, seuil_groupe_tags=0.8):\n",
    "    debut0 = datetime.datetime.now()\n",
    "    debut = datetime.datetime.now()\n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2)\n",
    "    \n",
    "    # Generation Encodage \n",
    "    genere_encodage(seuil=seuil_groupe_tags)\n",
    "    \n",
    "    # 0 - Encodage des Y selon le seuil\n",
    "    Y_train_name_encode = encode_Y(Y_train)\n",
    "    Y_test_name_encode = encode_Y(Y_test)\n",
    "\n",
    "    # 1 - Transfo TF-IDF VECTORIZER\n",
    "    nb_topics = Y_train_name_encode.nunique()\n",
    "    print('nb_topics = ', nb_topics)\n",
    "    \n",
    "    # On entraine la modelisation LDA\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    from sklearn.decomposition import NMF\n",
    "\n",
    "    # Run NMF\n",
    "    mnf = NMF(n_components=nb_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd')\n",
    "    \n",
    "    debut = datetime.datetime.now()\n",
    "    X_train_mnf_trained = mnf.fit_transform(X_train2)\n",
    "    fin = datetime.datetime.now()\n",
    "    print('Tps de Traitement : X_train_lda_trained\\t{}'.format(fin-debut))\n",
    "    \n",
    "    debut = datetime.datetime.now()\n",
    "    # On tranforme la données de test\n",
    "    X_test_mnf_trained = mnf.fit_transform(X_test2)\n",
    "    fin = datetime.datetime.now()\n",
    "    print('Tps de Traitement : X_test_lda_trained\\t{}'.format(fin-debut))\n",
    "    \n",
    "    debut = datetime.datetime.now()\n",
    "    print('\\t[ ETAPE3 ]')\n",
    "   \n",
    "    # On va standardiser les données\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    #std = StandardScaler()\n",
    "    #std.fit(np.concatenate((X_train_mnf_trained,X_test_mnf_trained)))\n",
    "    ########X_train_mnf_trained = std.transform(X_train_mnf_trained)\n",
    "    ########X_test_mnf_trained = std.transform(X_test_mnf_trained)\n",
    "\n",
    "    ## Entrainement par le Classifier\n",
    "    clf.fit(X_train_mnf_trained, Y_train_name_encode)\n",
    "    fin = datetime.datetime.now()\n",
    "    print('Tps de Traitement : clf.fit\\t{}'.format(fin-debut))\n",
    "    debut = datetime.datetime.now()\n",
    "\n",
    "    # On mesure le score\n",
    "    score_clf = clf.score(X_test_mnf_trained, Y_test_name_encode)\n",
    "    print('\\t-> score = ', score_clf)\n",
    "    \n",
    "    fin = datetime.datetime.now()\n",
    "    print('Tps de Traitement : SCORE\\t{}'.format(fin-debut))\n",
    "    print('\\nTps Total {}'.format(fin-debut0))\n",
    "    \n",
    "    return clf, clf.predict(X_test_mnf_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_topics =  66\n",
      "Tps de Traitement : X_train_lda_trained\t0:00:31.409727\n",
      "Tps de Traitement : X_test_lda_trained\t0:00:03.903934\n",
      "\t[ ETAPE3 ]\n",
      "Tps de Traitement : clf.fit\t0:00:02.672424\n",
      "\t-> score =  0.4346\n",
      "Tps de Traitement : SCORE\t0:00:00.007964\n",
      "\n",
      "Tps Total 0:00:56.935946\n",
      "\tPERFORMANCE MODELE : 56.8531  -- soit 0.9802 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf1 = LogisticRegression(C=0.10, penalty='l2', tol=0.1,random_state=0, \n",
    "                          solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "n_clf, y_pred_encoded = modelisation_MNF(\n",
    "    clf1, \n",
    "    X_train2=X_train2, X_test2=X_test2, Y_train=Y_train, Y_test=Y_test,\n",
    "    seuil_groupe_tags=0.8\n",
    ")\n",
    "\n",
    "mesure_performance(y_pred_encoded, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_topics =  66\n",
      "Tps de Traitement : X_train_lda_trained\t0:00:33.253386\n",
      "Tps de Traitement : X_test_lda_trained\t0:00:04.248784\n",
      "\t[ ETAPE3 ]\n",
      "Tps de Traitement : clf.fit\t0:00:01.818632\n",
      "\t-> score =  0.4346\n",
      "Tps de Traitement : SCORE\t0:00:00.009006\n",
      "\n",
      "Tps Total 0:00:58.013585\n",
      "\tPERFORMANCE MODELE : 56.8531  -- soit 0.9802 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf1 = LogisticRegression(C=0.01, penalty='l2', tol=0.1,random_state=0, \n",
    "                          solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "n_clf, y_pred_encoded = modelisation_MNF(\n",
    "    clf1, \n",
    "    X_train2=X_train2, X_test2=X_test2, Y_train=Y_train, Y_test=Y_test,\n",
    "    seuil_groupe_tags=0.8\n",
    ")\n",
    "\n",
    "mesure_performance(y_pred_encoded, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement d'un modèle par RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seb/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_topics =  66\n",
      "Tps de Traitement : X_train_lda_trained\t0:00:31.802162\n",
      "Tps de Traitement : X_test_lda_trained\t0:00:03.949348\n",
      "\t[ ETAPE3 ]\n",
      "Tps de Traitement : clf.fit\t0:01:09.166269\n",
      "\t-> score =  0.4336\n",
      "Tps de Traitement : SCORE\t0:00:01.958972\n",
      "\n",
      "Tps Total 0:02:05.695654\n",
      "\tPERFORMANCE MODELE : 56.85  -- soit 0.9802 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf2 = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)\n",
    "\n",
    "n_clf, y_pred_encoded = modelisation_MNF(\n",
    "    clf2, \n",
    "    X_train2=X_train2, X_test2=X_test2, Y_train=Y_train, Y_test=Y_test,\n",
    "    seuil_groupe_tags=0.8\n",
    ")\n",
    "\n",
    "mesure_performance(y_pred_encoded, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travail sur Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_title = df1.loc[X_train.index]['Title']\n",
    "X_test_title = df1.loc[X_test.index]['Title']\n",
    "\n",
    "vect_tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    min_df=100,\n",
    "    max_df=0.2,\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preProcess_keep_Code,\n",
    "    stop_words=StopWord,        # Pas de stopWord car nous les cherchons\n",
    "    ngram_range=(1, 6)\n",
    ")\n",
    "\n",
    "X_train_title2 = vect_tfidf.fit_transform(X_train_title)\n",
    "X_test_title2 = vect_tfidf.transform(X_test_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_topics =  66\n",
      "Tps de Traitement : X_train_lda_trained\t0:00:23.030431\n",
      "Tps de Traitement : X_test_lda_trained\t0:00:03.345411\n",
      "\t[ ETAPE3 ]\n",
      "Tps de Traitement : clf.fit\t0:00:01.424128\n",
      "\t-> score =  0.4346\n",
      "Tps de Traitement : SCORE\t0:00:00.008363\n",
      "\n",
      "Tps Total 0:00:46.608810\n",
      "\tPERFORMANCE MODELE : 56.8531  -- soit 0.9802 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression(C=0.01, penalty='l2', tol=0.1,random_state=0, \n",
    "                          solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "n_clf, y_pred_encoded_title = modelisation_MNF(\n",
    "    clf1, \n",
    "    X_train2=X_train_title2, X_test2=X_test_title2, Y_train=Y_train, Y_test=Y_test,\n",
    "    seuil_groupe_tags=0.8\n",
    ")\n",
    "\n",
    "mesure_performance(y_pred_encoded_title, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_topics =  66\n",
      "Tps de Traitement : X_train_lda_trained\t0:00:31.115794\n",
      "Tps de Traitement : X_test_lda_trained\t0:00:03.884413\n",
      "\t[ ETAPE3 ]\n",
      "Tps de Traitement : clf.fit\t0:00:01.511436\n",
      "\t-> score =  0.4346\n",
      "Tps de Traitement : SCORE\t0:00:00.007539\n",
      "\n",
      "Tps Total 0:00:55.038852\n",
      "\tPERFORMANCE MODELE : 56.8531  -- soit 0.9802 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf1 = LogisticRegression(C=0.01, penalty='l2', tol=0.1,random_state=0, \n",
    "                          solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "n_clf, y_pred_encoded_body = modelisation_MNF(\n",
    "    clf1, \n",
    "    X_train2=X_train2, X_test2=X_test2, Y_train=Y_train, Y_test=Y_test,\n",
    "    seuil_groupe_tags=0.8\n",
    ")\n",
    "\n",
    "mesure_performance(y_pred_encoded_body, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Si on applique un OR sur les Titre et Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPERFORMANCE MODELE : 56.8531  -- soit 0.9802 %\n"
     ]
    }
   ],
   "source": [
    "y_pred_encoded_title_OR_body = np.where(np.logical_or(y_pred_encoded_title, y_pred_encoded_body), 1, 0)\n",
    "mesure_performance(y_pred_encoded_title_OR_body, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pas de progression sur le résultat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
